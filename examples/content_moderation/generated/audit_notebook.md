# JUSTITIA Policy Generation Audit Notebook

**Domain:** content_moderation

**Generated:** 1.0

## Chain-of-Thought Reasoning

1. **Identify Core Concepts**  
   - *Hate speech* includes slurs, hateful symbols, or content targeting protected groups.  
   - *Harassment* comprises repeated personal attacks, threats, or intimidation.  
   - *Explicit content* covers nudity, graphic sexual acts, and extreme violence.  

2. **Determine Decision Criteria**  
   - For each category, we need clear, objective signals that can be programmatically detected.  
   - Criteria should be a list of patterns or semantic checks.  
   - Examples help clarify borderline cases.  

3. **Severity Levels**  
   - Hate speech and explicit content are high‑severity because they violate legal and ethical standards.  
   - Harassment is medium‑severity; it is harmful but often contextual.  

4. **Exceptions & Rationale**  
   - *Hate speech*: Academic, historical, or journalistic contexts may discuss hateful content.  
   - *Harassment*: Defensive self‑reporting or whistle‑blowing may include personal attacks.  
   - *Explicit content*: Medical, educational, or artistic contexts may legitimately contain explicit imagery.  

5. **Audit Trail**  
   - The JSON policy will include a “rationale” field for each rule and exception, explaining why the rule exists.  

6. **Test Cases**  
   - Provide concrete examples for each rule, labeled “allowed” or “prohibited.”  
   - Add a “test_cases” array in the policy for automated validation.  

7. **Structure**  
   - Domain: “content_moderation”.  
   - Version: “1.0”.  
   - Rules: three main rules with ids.  
   - Exceptions: linked to rule ids.  
   - Test_cases: list of message strings with expected outcome.

---

*Generated by JUSTITIA AI Policy Compiler*
